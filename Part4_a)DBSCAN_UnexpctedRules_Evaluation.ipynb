{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Part4:a)DBSCAN_UnexpctedRules_Evaluation.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOqNcsEUnHA3tazo53NqdwB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BlackCurrantDS/ADTB/blob/master/Part4_a)DBSCAN_UnexpctedRules_Evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHUFr3H57-NJ"
      },
      "source": [
        "This Notebook test the unexpected rules with different classifiers.\r\n",
        "\r\n",
        "How to run:\r\n",
        "\r\n",
        "Chnage the train and test file name and path in cell 1.\r\n",
        "\r\n",
        "Please make sure the file \"unexpected_rule_file_json\" is in the same folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NjQp_lM97YS"
      },
      "source": [
        "inputfilepath = \"/content/breast_train_transactions.txt\"\r\n",
        "inputTestfilepath = \"/content/breast_test_transactions.txt\"\r\n",
        "file_path = \"/content/\"\r\n",
        "clus = \"DBSCAN\""
      ],
      "execution_count": 354,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqZgiZaL49B2"
      },
      "source": [
        "from sklearn.metrics.ranking import roc_curve, auc\r\n",
        "from sklearn.metrics.classification import f1_score\r\n",
        "from sklearn.svm.classes import SVC\r\n",
        "from sklearn.ensemble.forest import RandomForestClassifier"
      ],
      "execution_count": 355,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzkyrBLLFfNs"
      },
      "source": [
        "\r\n",
        "\r\n",
        "class AssociationRule:\r\n",
        "    def __init__(self, left, right):\r\n",
        "        self.left_items = left\r\n",
        "        self.right_items = right\r\n",
        "        self.scores = []\r\n",
        "        \r\n",
        "    def length(self):\r\n",
        "        return len(self.left_items) + len(self.right_items)\r\n",
        "     \r\n",
        "    def score(self, index):\r\n",
        "        return self.scores[index]\r\n",
        "    \r\n",
        "    def lhs_string(self):\r\n",
        "        return itemset_2_string(self.left_items)\r\n",
        "        \r\n",
        "    def rhs_string(self):\r\n",
        "        return itemset_2_string(self.right_items)\r\n",
        "    \r\n",
        "    def serialize(self):\r\n",
        "        left_key = self.lhs_string()\r\n",
        "        right_key = self.rhs_string()\r\n",
        "        return left_key + \">\" + right_key\r\n",
        "    \r\n",
        "    @staticmethod        \r\n",
        "    def string_2_rule(s):\r\n",
        "        subStrings = s.split(\">\")\r\n",
        "        left = string_2_itemset(subStrings[0].strip())\r\n",
        "        right = string_2_itemset(subStrings[1].strip())\r\n",
        "        #print(\"AssociationRule(left, right\",AssociationRule(left, right))\r\n",
        "        return AssociationRule(left, right)\r\n",
        "\r\n",
        "    def append_score(self, score):\r\n",
        "        self.scores.append(score)\r\n",
        "        \r\n",
        "    def get_itemset(self):\r\n",
        "        itemset = []\r\n",
        "        itemset.extend(self.left_items)\r\n",
        "        itemset.extend(self.right_items)\r\n",
        "        itemset.sort()\r\n",
        "        return itemset\r\n",
        "        \r\n",
        "        \r\n",
        "    def rule_itemset_2_string(self):\r\n",
        "        itemset = self.get_itemset()\r\n",
        "        return itemset_2_string(itemset)\r\n",
        "    \r\n",
        "    def compute_basic_probs(self,frequent_itemsets, nTransactions):  \r\n",
        "        \r\n",
        "        left = frequent_itemsets[self.lhs_string()]\r\n",
        "        right = frequent_itemsets[self.rhs_string()]\r\n",
        "        \r\n",
        "        both = frequent_itemsets[self.rule_itemset_2_string()]\r\n",
        "        \r\n",
        "        vector = {}\r\n",
        "        \r\n",
        "        ''' 1. P(A)'''\r\n",
        "        p_A = left/nTransactions\r\n",
        "        vector['A'] = p_A\r\n",
        "        \r\n",
        "        ''' 2. P(B)'''\r\n",
        "        p_B = right/nTransactions\r\n",
        "        vector['B'] = p_B\r\n",
        "        \r\n",
        "        ''' 3. P(~A)'''\r\n",
        "        p_not_A = 1 - p_A\r\n",
        "        vector['~A'] = p_not_A\r\n",
        "        \r\n",
        "        ''' 4. P(~B)'''\r\n",
        "        p_not_B = 1 - p_B\r\n",
        "        vector['~B'] = p_not_B\r\n",
        "        \r\n",
        "        ''' 5. P(AB) '''\r\n",
        "        p_A_and_B = both/nTransactions\r\n",
        "        vector['AB'] = p_A_and_B\r\n",
        "        \r\n",
        "        ''' 6. P(~AB)'''\r\n",
        "        p_not_A_and_B = (right - both)/nTransactions\r\n",
        "        vector['~AB'] = p_not_A_and_B\r\n",
        "        \r\n",
        "        ''' 7. P(A~B)'''\r\n",
        "        p_A_and_not_B = (left - both)/nTransactions\r\n",
        "        vector['A~B'] = p_A_and_not_B\r\n",
        "        \r\n",
        "        ''' 8. P(~A~B)'''\r\n",
        "        p_not_A_and_not_B = 1 - (left + right - both)/nTransactions\r\n",
        "        vector['~A~B'] = p_not_A_and_not_B \r\n",
        "        \r\n",
        "        '''\r\n",
        "        9. P(A|B)\r\n",
        "        '''\r\n",
        "        p_A_if_B = p_A_and_B / p_B\r\n",
        "        vector['A|B'] = p_A_if_B\r\n",
        "        \r\n",
        "        '''\r\n",
        "        10. P(~A|~B)\r\n",
        "        '''\r\n",
        "        p_not_A_if_not_B = p_not_A_and_not_B / p_not_B\r\n",
        "        vector['~A|~B'] = p_not_A_if_not_B\r\n",
        "        \r\n",
        "        '''\r\n",
        "        11. P(A|~B)\r\n",
        "        '''\r\n",
        "        p_A_if_not_B = p_A_and_not_B/p_not_B\r\n",
        "        vector['A|~B'] = p_A_if_not_B\r\n",
        "        \r\n",
        "        '''\r\n",
        "        12. p(~A|B)\r\n",
        "        '''\r\n",
        "        p_not_A_if_B = p_not_A_and_B / p_B\r\n",
        "        vector['~A|B'] = p_not_A_if_B\r\n",
        "        \r\n",
        "        '''\r\n",
        "        13. P(B|A)\r\n",
        "        '''\r\n",
        "        p_B_if_A = p_A_and_B / p_A\r\n",
        "        vector['B|A'] = p_B_if_A\r\n",
        "        \r\n",
        "        '''\r\n",
        "        14. P(~B|~A)\r\n",
        "        '''\r\n",
        "        p_not_B_if_not_A = p_not_A_and_not_B / p_not_A\r\n",
        "        vector['~B|~A'] = p_not_B_if_not_A\r\n",
        "        \r\n",
        "        '''\r\n",
        "        15. P(B|~A)\r\n",
        "        '''\r\n",
        "        p_B_if_not_A = p_not_A_and_B/p_not_A\r\n",
        "        vector['B|~A'] = p_B_if_not_A\r\n",
        "        \r\n",
        "        '''\r\n",
        "        16. p(~B|A)\r\n",
        "        '''\r\n",
        "        p_not_B_if_A = p_A_and_not_B / p_A\r\n",
        "        vector['~B|A'] = p_not_B_if_A\r\n",
        "        \r\n",
        "        return vector\r\n",
        "    \r\n",
        "    def is_redundant_(self, bits, k, itemset, freq_itemset_dict): \r\n",
        "        '''\r\n",
        "        Run out of items --> create rule and check format criterion\r\n",
        "        '''\r\n",
        "        if k >= len(itemset):\r\n",
        "            items_1 = []\r\n",
        "            items_2 = []\r\n",
        "            for index in range(len(bits)):\r\n",
        "                if bits[index] == True:\r\n",
        "                    items_1.append(itemset[index])\r\n",
        "                else:\r\n",
        "                    items_2.append(itemset[index])\r\n",
        "            for item in items_2:\r\n",
        "                rule = AssociationRule(items_1, [item])\r\n",
        "                confidence = freq_itemset_dict.getConfidence(rule)\r\n",
        "                if confidence == 1: return True\r\n",
        "            return False \r\n",
        "      \r\n",
        "        value_domain = [True, False]\r\n",
        "        for value in value_domain:\r\n",
        "            bits[k] = value\r\n",
        "            checker = self.is_redundant_(bits, k+1, itemset, freq_itemset_dict)\r\n",
        "            if checker == True: return True\r\n",
        "            bits[k] = True    \r\n",
        "        return False\r\n",
        "    \r\n",
        "    '''\r\n",
        "    Expand an item-set with equivalent items.\r\n",
        "    '''\r\n",
        "    def is_redundant(self, freq_itemset_dict):\r\n",
        "        bits = [True for _ in self.left_items]\r\n",
        "        checker = self.is_redundant_(bits, 0, self.left_items, freq_itemset_dict)\r\n",
        "        if checker == True: return True\r\n",
        "        \r\n",
        "        bits =  [True for _ in self.right_items]\r\n",
        "        return self.is_redundant_(bits, 0, self.right_items, freq_itemset_dict)\r\n",
        "    \r\n",
        "    '''\r\n",
        "    Check if an item-set is satisfied condition of the rule. \r\n",
        "    '''\r\n",
        "    def satisfy_rule(self, itemset, is_lhs = True):\r\n",
        "        condition = self.left_items\r\n",
        "        print(\"condition\",condition)\r\n",
        "        if is_lhs == False: condition = self.right_items\r\n",
        "        print(\"is_lhs\",is_lhs)\r\n",
        "        print(\"condition\",condition)\r\n",
        "        print(\"len(condition)\",len(condition))\r\n",
        "        print(\"len(itemset)\",len(itemset))\r\n",
        "        if len(condition) > len(itemset) or len(itemset) == 0:\r\n",
        "            return False\r\n",
        "        for item in condition:\r\n",
        "            print(\"item\",item)\r\n",
        "            if item not in itemset:\r\n",
        "                print(\"this item is not in itemset\", item)\r\n",
        "                return False\r\n",
        "        return True\r\n",
        "    "
      ],
      "execution_count": 356,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zbtvr7dmDf6h"
      },
      "source": [
        "\r\n",
        "\r\n",
        "class RelationArray2D(object):\r\n",
        "    '''\r\n",
        "    classdocs\r\n",
        "    '''\r\n",
        "\r\n",
        "    def __init__(self, item_dict, relation_values):\r\n",
        "        '''\r\n",
        "        Constructor\r\n",
        "        '''\r\n",
        "        self.item_dict = item_dict\r\n",
        "        self.relation_matrix = relation_values\r\n",
        "        \r\n",
        "        \r\n",
        "    def get_value(self, item1, item2):\r\n",
        "        i = self.item_dict[item1]\r\n",
        "        j = self.item_dict[item2]\r\n",
        "        return self.relation_matrix[i, j]\r\n",
        "    \r\n",
        "    def get_items(self):\r\n",
        "        return self.item_dict.keys()\r\n",
        "        \r\n",
        "    def get_index(self, item):\r\n",
        "        return self.item_dict[item]\r\n",
        "    \r\n",
        "class RelationArray1D(object):\r\n",
        "    '''\r\n",
        "    classdocs\r\n",
        "    '''\r\n",
        "\r\n",
        "    def __init__(self, item_dict, values):\r\n",
        "        '''\r\n",
        "        Constructor\r\n",
        "        '''\r\n",
        "        self.item_dict = item_dict\r\n",
        "        self.values = values\r\n",
        "        \r\n",
        "        \r\n",
        "    def get_value_at(self, index):\r\n",
        "        return self.values[index]\r\n",
        "    \r\n",
        "    def get_items(self):\r\n",
        "        return self.item_dict.keys()\r\n",
        "        \r\n",
        "    def get_value(self, item):\r\n",
        "        return self.item_dict[item]"
      ],
      "execution_count": 357,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRTjXpJE6P01"
      },
      "source": [
        "# Transaction databases, each transaction is a set of items\r\n",
        "import numpy as np\r\n",
        "from scipy import sparse\r\n",
        "from scipy import stats\r\n",
        "\r\n",
        "\r\n",
        "class DataSet:\r\n",
        "    def __init__(self):\r\n",
        "        self.current = 0\r\n",
        "        self.train_data = []\r\n",
        "        self.data_labels = []\r\n",
        "        \r\n",
        "    \r\n",
        "    def __iter__(self):\r\n",
        "        return iter(self.train_data)\r\n",
        "                \r\n",
        "    def size(self):\r\n",
        "        return len(self.train_data)\r\n",
        "    \r\n",
        "    def get_transaction(self, index):\r\n",
        "        return self.train_data[index]\r\n",
        "    \r\n",
        "    def clear(self):\r\n",
        "        self.train_data.clear()\r\n",
        "        \r\n",
        "    def add_transaction(self, t):\r\n",
        "        return self.train_data.append(t) \r\n",
        "        \r\n",
        "    '''\r\n",
        "    Load data set from a file. The input file must be formated in CSV (comma separated)\r\n",
        "    class_index is used in the case of data-set with labels. \r\n",
        "    '''\r\n",
        "    def load(self, file_path, class_index = -1, has_header = False):\r\n",
        "        self.train_data = []\r\n",
        "        if class_index != -1: self.data_labels = []\r\n",
        "        \r\n",
        "        with open(file_path, \"r\") as text_in_file:\r\n",
        "            if has_header == True:\r\n",
        "                text_in_file.readline()\r\n",
        "                \r\n",
        "            for line in text_in_file:\r\n",
        "                #print(\"dataset script line\", line)\r\n",
        "                transaction = [x.strip() for x in line.split(',')]\r\n",
        "                transaction = list(filter(None, transaction))\r\n",
        "                #print(\"datset script transaction\" , transaction)\r\n",
        "                \r\n",
        "                if (class_index != -1):\r\n",
        "                    self.data_labels.append(transaction[class_index])\r\n",
        "                    del transaction[class_index]\r\n",
        "                \r\n",
        "                self.train_data.append(list(set(transaction)))\r\n",
        "        print(\"Loading done\")\r\n",
        "    '''\r\n",
        "    Return number of classes in data (if have).\r\n",
        "    '''            \r\n",
        "    def number_of_classes(self):\r\n",
        "        if self.data_labels == None: return 0\r\n",
        "        return len(set(self.data_labels))\r\n",
        "\r\n",
        "    def convert_data_labels(self, inlier_name):\r\n",
        "        Y_train = np.zeros(len(self.data_labels))\r\n",
        "        for i in range(Y_train.shape[0]):\r\n",
        "            if self.data_labels[i] == inlier_name:\r\n",
        "                Y_train[i] = 1\r\n",
        "            else: \r\n",
        "                Y_train[i] = -1\r\n",
        "        return Y_train\r\n",
        "\r\n",
        "    def convert_2_binary_format_with(self, items_dict, classes_dict = None):\r\n",
        "        n_items = len(items_dict)\r\n",
        "        X_train = np.zeros((self.size(), n_items))\r\n",
        "        \r\n",
        "        k = 0\r\n",
        "        for transaction in self.train_data:\r\n",
        "            for item in transaction:\r\n",
        "                if item not in items_dict: \r\n",
        "                    print('not in features...')\r\n",
        "                    continue\r\n",
        "                i = items_dict[item]\r\n",
        "                X_train[k, i] = 1.0\r\n",
        "            k += 1\r\n",
        "            \r\n",
        "        Y_train = []\r\n",
        "        if classes_dict is not None:\r\n",
        "            for label in self.data_labels:\r\n",
        "                if label not in classes_dict:\r\n",
        "                    print('not in classes')\r\n",
        "                    Y_train.append(-1)\r\n",
        "                else:\r\n",
        "                    Y_train.append(classes_dict[label]) \r\n",
        "        return X_train, np.array(Y_train)        \r\n",
        "    \r\n",
        "    def get_items_dict_(self):\r\n",
        "        attr_dict = {}\r\n",
        "        #check existing data\r\n",
        "        for transaction in self.train_data:\r\n",
        "            for index in range (len(transaction)):\r\n",
        "                item_name = transaction[index]\r\n",
        "                if item_name not in attr_dict:\r\n",
        "                    attr_dict[item_name] = True\r\n",
        "        return attr_dict\r\n",
        "    \r\n",
        "    def get_class_list_(self):\r\n",
        "        # Sort items and classes in alphabet order.\r\n",
        "        return sorted(set(self.data_labels))\r\n",
        "        \r\n",
        "        \r\n",
        "\r\n",
        "    '''\r\n",
        "    Convert transaction data into binary format\r\n",
        "    '''\r\n",
        "    def convert_2_binary_format(self):\r\n",
        "        \r\n",
        "        attr_dict = self.get_items_dict_()\r\n",
        "        print(\"convert_2_binary_format attr_dict\", attr_dict)\r\n",
        "        \"\"\"\r\n",
        "        {'a6@3': True, 'a7@left': True, 'a9@no': True, 'a3@30-34': True, 'a1@30-39': True, 'class@no': True, 'a5@no': True, 'a4@0-2': True, 'a2@premeno': True, 'a8@left_low': True, 'a8@right_up': True, 'a3@20-24': True, 'a7@right': True, 'a1@40-49': True, 'a6@2': True, 'a8@left_up': True, 'a2@ge40': True, 'a1@60-69': True, 'a3@15-19': True, 'a8@right_low': True, 'a3@0-4': True, 'a3@25-29': True, 'a1@50-59': True, 'a3@50-54': True, 'a8@central': True, 'a6@1': True, 'a3@10-14': True, 'a2@lt40': True, 'a3@40-44': True, 'a3@35-39': True, 'a1@70-79': True, 'a3@5-9': True, 'a9@yes': True, 'a5@yes': True, 'a4@6-8': True, 'a4@9-11': True, 'a4@3-5': True, 'a3@45-49': True, 'a5@?': True, 'a4@15-17': True, 'a4@12-14': True, 'class@yes': True, 'a8@?': True, 'a4@24-26': True}\r\n",
        "        \r\n",
        "        \r\n",
        "        \"\"\"\r\n",
        "         \r\n",
        "        # Sort items and classes in alphabet order.\r\n",
        "        classes_list = sorted(set(self.data_labels))\r\n",
        "        items_list = sorted(attr_dict.keys())\r\n",
        "        \r\n",
        "        classes_dict = {classes_list[i] : i for i in range(len(classes_list))}\r\n",
        "        attr_dict = {items_list[i] : i for i in range(len(items_list))}\r\n",
        "        \r\n",
        "        #Generate binary matrix (X_train) and array of labels(Y_train)\r\n",
        "        X_train, Y_train = self.convert_2_binary_format_with(attr_dict, classes_dict)\r\n",
        "                \r\n",
        "        return RelationArray2D(attr_dict, sparse.csr_matrix(X_train)), RelationArray1D(classes_dict, np.array(Y_train))\r\n",
        "        \r\n",
        "    @staticmethod\r\n",
        "    def write_relation_matrix_(matrix):\r\n",
        "        with open(file_path+'item_relation.csv', 'w') as file_writer:\r\n",
        "            item_names = sorted(matrix.item_dict.keys())\r\n",
        "            file_writer.write('o0o,')\r\n",
        "            file_writer.write(','.join(item_names))\r\n",
        "            file_writer.write('\\n')\r\n",
        "            for i in range(len(item_names)):\r\n",
        "                file_writer.write(item_names[i] + ',')\r\n",
        "                file_writer.write(','.join(str(x) for x in matrix.relation_matrix[i].tolist()))\r\n",
        "                file_writer.write('\\n')\r\n",
        "                \r\n",
        "                \r\n",
        "   \r\n",
        "    '''\r\n",
        "    This method estimates relationship among items. There're two kinds of relationship\r\n",
        "    - Correlation:including negative correlation (<= -0.3) and positive correlation (>= 0.3)\r\n",
        "    - Cover: threshold 1.0, including cover (2) and covered (-2) \r\n",
        "    '''\r\n",
        "    def items_relationship(self):\r\n",
        "        \r\n",
        "        print ('Computing item relation matrix...')\r\n",
        "        \r\n",
        "        X_train, _ = self.convert_2_binary_format()\r\n",
        "        \r\n",
        "        print(\"X_train\",X_train)\r\n",
        "    \r\n",
        "        correlation_matrix, p_values = stats.spearmanr(X_train.relation_matrix.todense(), axis = 0)\r\n",
        "        \r\n",
        "        zeros_mask = (p_values <= 0.05).astype(int)\r\n",
        "        small_mask = (np.abs(correlation_matrix) >= 0.1).astype(int)\r\n",
        "        \r\n",
        "        relation_matrix = correlation_matrix * small_mask * zeros_mask\r\n",
        "        \r\n",
        "        a = RelationArray2D(X_train.item_dict, relation_matrix)\r\n",
        "        DataSet.write_relation_matrix_(a)\r\n",
        "        \r\n",
        "        return a"
      ],
      "execution_count": 358,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQzXb6vnDL76"
      },
      "source": [
        "train_data_set = DataSet()"
      ],
      "execution_count": 359,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eiJEAVwYDo4a",
        "outputId": "9c5aa254-d0dd-449e-c2a2-f27fc71ecb6f"
      },
      "source": [
        "train_data_set.load(inputfilepath, 0, has_header = False)"
      ],
      "execution_count": 360,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIdEQT-EDUd5",
        "outputId": "7051b84f-da95-46f6-8ce8-21da6a9d664c"
      },
      "source": [
        "X_train, Y_train = train_data_set.convert_2_binary_format()"
      ],
      "execution_count": 361,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "convert_2_binary_format attr_dict {'a6@3': True, 'a1@30-39': True, 'a7@left': True, 'a2@premeno': True, 'a3@30-34': True, 'a4@0-2': True, 'a5@no': True, 'a9@no': True, 'a8@left_low': True, 'a1@40-49': True, 'a3@20-24': True, 'a7@right': True, 'a8@right_up': True, 'a6@2': True, 'a2@ge40': True, 'a1@60-69': True, 'a3@15-19': True, 'a8@left_up': True, 'a3@0-4': True, 'a8@right_low': True, 'a3@25-29': True, 'a1@50-59': True, 'a3@50-54': True, 'a8@central': True, 'a3@10-14': True, 'a6@1': True, 'a2@lt40': True, 'a3@40-44': True, 'a3@35-39': True, 'a1@70-79': True, 'a3@5-9': True, 'a5@yes': True, 'a4@6-8': True, 'a9@yes': True, 'a4@9-11': True, 'a4@3-5': True, 'a3@45-49': True, 'a5@?': True, 'a4@15-17': True, 'a4@12-14': True, 'a8@?': True, 'a4@24-26': True}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywchwuGZcKVw",
        "outputId": "e3ce0b0a-4216-4211-f2fb-5cb7c13d72fd"
      },
      "source": [
        "Y_train.item_dict"
      ],
      "execution_count": 362,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'class@no': 0, 'class@yes': 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 362
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7n9R_JXDy24",
        "outputId": "06e7519c-f79f-448e-bdc3-f10bcfcec15c"
      },
      "source": [
        "test_data_set = DataSet()\r\n",
        "test_data_set.load(inputTestfilepath, 0, has_header = False)\r\n",
        "Xtest, Ytest = test_data_set.convert_2_binary_format_with(X_train.item_dict, Y_train.item_dict)\r\n",
        "Ytest = Ytest.flatten()"
      ],
      "execution_count": 363,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading done\n",
            "not in features...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEPvCOaNEC_b"
      },
      "source": [
        "class_count = train_data_set.number_of_classes()"
      ],
      "execution_count": 364,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2zxJATYczXR",
        "outputId": "4b12e305-62f3-4c79-955e-fa08786d8731"
      },
      "source": [
        "class_count"
      ],
      "execution_count": 365,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 365
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yN-eQoEiHKd5"
      },
      "source": [
        "import json\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "class IOHelper:\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def write_file_in_lines(file_name, data, header = None):\r\n",
        "        with open(file_name, \"w\") as text_file:\r\n",
        "            if header is not None:\r\n",
        "                text_file.write(header)\r\n",
        "                text_file.write('\\n')\r\n",
        "            for transaction in data:\r\n",
        "                text_file.write(transaction)\r\n",
        "                text_file.write('\\n')\r\n",
        "    \r\n",
        "    @staticmethod        \r\n",
        "    def read_file_in_lines(inputfile, has_header = False):\r\n",
        "        data = []\r\n",
        "        with open(inputfile, \"r\") as text_file:\r\n",
        "            file_iter = iter(text_file)\r\n",
        "            if has_header == True:\r\n",
        "                next(file_iter)\r\n",
        "            \r\n",
        "            for line in file_iter:\r\n",
        "                data.append(line.strip())\r\n",
        "        return data\r\n",
        "    \r\n",
        "    @staticmethod\r\n",
        "    def read_ranking_file(input_file):\r\n",
        "        patterns = []\r\n",
        "        ranking = []\r\n",
        "        k = 0\r\n",
        "        with open(input_file, \"r\") as text_file:\r\n",
        "            for line in text_file:\r\n",
        "                subStrings = line.split(';')\r\n",
        "                rule_key = subStrings[0].strip()\r\n",
        "                patterns.append(rule_key)\r\n",
        "                ranking.append([])\r\n",
        "                for v in subStrings[1:]:\r\n",
        "                    r = int(v)\r\n",
        "                    ranking[k].append(r)\r\n",
        "                \r\n",
        "                k += 1\r\n",
        "                if k % 1000 == 0: print(str(k))\r\n",
        "        return patterns, np.array(ranking)\r\n",
        "    \r\n",
        "    @staticmethod \r\n",
        "    def save_as_json_format(file_name, o):\r\n",
        "        with open (file_name, 'w') as text_file:\r\n",
        "            json.dump(o, text_file)\r\n",
        "            \r\n",
        "    @staticmethod \r\n",
        "    def save_as_json_format_in_line(file_name, o):\r\n",
        "        with open (file_name, 'w') as text_file:\r\n",
        "            #json.dump(o, text_file)\r\n",
        "            for item in o:\r\n",
        "                line = json.dumps(item)\r\n",
        "                text_file.write(line)\r\n",
        "                text_file.write('\\n')\r\n",
        "            \r\n",
        "    @staticmethod        \r\n",
        "    def load_json_object(file_name):\r\n",
        "        with open(file_name, 'r') as text_file:\r\n",
        "            o = json.load(text_file)\r\n",
        "            return o\r\n",
        "    \r\n",
        "    @staticmethod    \r\n",
        "    def write_matrix(file_name, matrix):\r\n",
        "        with open(file_name, \"w\") as text_file:\r\n",
        "            for line in matrix:\r\n",
        "                text_file.write(','.join(str(x) for x in line.tolist()))\r\n",
        "                text_file.write('\\n')\r\n",
        "    \r\n",
        "    @staticmethod\r\n",
        "    def write_list_of_tuples(file_name, tuples_list):\r\n",
        "        with open(file_name, 'w') as writer:\r\n",
        "            for rule in tuples_list:\r\n",
        "                writer.write(str(rule))\r\n",
        "                writer.write('\\n')"
      ],
      "execution_count": 366,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1v3mHEjE1Gy"
      },
      "source": [
        "unexpected_rules = IOHelper.load_json_object(file_path+\"unexpected_rule_file_json\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sY7PJZyxFVrK"
      },
      "source": [
        "def filter_association_rules(unexpected_rules, delta_1 = 0):\r\n",
        "    rules = []\r\n",
        "    for x in unexpected_rules:\r\n",
        "        if x[2][0][1] > delta_1: \r\n",
        "            rules.append(AssociationRule.string_2_rule(x[0]))\r\n",
        "    return rules"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUKjZ3PQIs-X"
      },
      "source": [
        "def string_2_itemset(key):\r\n",
        "    if key == '':\r\n",
        "        return []\r\n",
        "    else: \r\n",
        "        return key.split(',')\r\n",
        "\r\n",
        "def itemset_2_string(itemset):\r\n",
        "    return \",\".join(itemset)\r\n",
        "\r\n",
        "def merge_itemsets(itemset_1, itemset_2):\r\n",
        "    merged_items = []\r\n",
        "    merged_items.extend(itemset_1)\r\n",
        "    merged_items.extend(itemset_2)\r\n",
        "    merged_items = list(set(merged_items))\r\n",
        "    merged_items = sorted(merged_items)\r\n",
        "    \r\n",
        "    return merged_items\r\n",
        "\r\n",
        "def get_full_path(prefix, file_name):\r\n",
        "    if prefix == '': return file_name\r\n",
        "    return prefix + '//' + file_name\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ztudi2TEHQZ"
      },
      "source": [
        "refined_unexpected_rules = filter_association_rules(unexpected_rules)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyJDde0kI0Xm"
      },
      "source": [
        "print('svm testing...')\r\n",
        "svc_model = SVC()\r\n",
        "svc_model.fit(X_train.relation_matrix, Y_train.values.flatten())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sb8u3UFwJSJH"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\r\n",
        "import seaborn as sns\r\n",
        "import pandas as pd\r\n",
        "from sklearn.metrics import plot_confusion_matrix\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "def refine_with_unexpectedness(data_set, classes_dict, preY, Ytrue, unexpected_rules, model, clus):\r\n",
        "    \r\n",
        "    print('Refine with unexpected rules...')\r\n",
        "    y_pred = np.copy(preY)\r\n",
        "    for i in range(data_set.size()):\r\n",
        "        x = data_set.get_transaction(i)\r\n",
        "        print(\"x is \",x)\r\n",
        "        for r in unexpected_rules:\r\n",
        "            #print(r)\r\n",
        "            if r.satisfy_rule(x, is_lhs = True):\r\n",
        "                print(\"r.satisfy_rule(x, is_lhs = True)\", r.satisfy_rule(x, is_lhs = True))\r\n",
        "                label = r.right_items[0]\r\n",
        "                print(\"label is \",label)\r\n",
        "                y_pred[i] = classes_dict[label]\r\n",
        "                print(\"y pred for i is\",y_pred[i])\r\n",
        "                \r\n",
        "    print(f1_score(Ytrue, y_pred, average=None))\r\n",
        "    print(confusion_matrix(Ytrue, y_pred))\r\n",
        "    c=confusion_matrix(Ytrue, y_pred)\r\n",
        "    df_cm = pd.DataFrame(c, range(2), range(2))\r\n",
        "    sns.set(font_scale=1.4) # for label size\r\n",
        "    sns.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}) # font size\r\n",
        "\r\n",
        "    \r\n",
        "    if (data_set.number_of_classes() <= 2):\r\n",
        "        fpr, tpr, _ = roc_curve(Ytrue, y_pred.flatten())\r\n",
        "        print(auc(fpr, tpr))\r\n",
        "\r\n",
        "    plt.savefig(file_path+\"Refined_\"+model+\"_\"+clus+\"_\"+\"confusion_matrix.png\")\r\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JenXJwU1JFIW"
      },
      "source": [
        "SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3QnrJyGI-22"
      },
      "source": [
        "svc_y_pred = svc_model.predict(Xtest)\r\n",
        "print(f1_score(Ytest, svc_y_pred, average=None))\r\n",
        "if (class_count <= 2):\r\n",
        "  fpr, tpr, _ = roc_curve(Ytest, svc_y_pred.flatten())    \r\n",
        "  print(auc(fpr, tpr))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRRNocITb-Gs"
      },
      "source": [
        "#Y_train.item_dict = {'class@no': 0, 'class@yes': 1}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n98RxYOvJLK3"
      },
      "source": [
        "refine_with_unexpectedness(test_data_set, Y_train.item_dict, svc_y_pred, Ytest, refined_unexpected_rules, \"SVM\", clus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEHKZ9L04Qh1"
      },
      "source": [
        "class_names = ['yes', 'no']\r\n",
        "# Plot non-normalized confusion matrix\r\n",
        "plot_confusion_matrix(svc_model, Xtest, Ytest,\r\n",
        "                                 display_labels=class_names,\r\n",
        "                                 cmap=plt.cm.Blues,\r\n",
        "                                 )\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJlBLIOhJGOf"
      },
      "source": [
        "Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UukCWLYsJIJ-"
      },
      "source": [
        "print('Random forest testing...')\r\n",
        "rf_model = RandomForestClassifier(n_estimators=20, random_state=1)\r\n",
        "rf_model.fit(X_train.relation_matrix, Y_train.values.flatten())\r\n",
        "    \r\n",
        "rf_y_pred = rf_model.predict(Xtest)\r\n",
        "print(f1_score(Ytest, rf_y_pred, average=None))\r\n",
        "if (class_count <= 2):\r\n",
        "  fpr, tpr, _ = roc_curve(Ytest, rf_y_pred.flatten())\r\n",
        "  print(auc(fpr, tpr))\r\n",
        "    \r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mo9p57d7rOt"
      },
      "source": [
        "class_names = ['yes', 'no']\r\n",
        "# Plot non-normalized confusion matrix\r\n",
        "plot_confusion_matrix(rf_model, Xtest, Ytest,\r\n",
        "                                 display_labels=class_names,\r\n",
        "                                 cmap=plt.cm.Blues,\r\n",
        "                                 )\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQTAB9Lv7s90"
      },
      "source": [
        "refine_with_unexpectedness(test_data_set, Y_train.item_dict, rf_y_pred, Ytest, refined_unexpected_rules, \"RF\", clus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSQcPASNJh-g"
      },
      "source": [
        "MLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noswGDE6Ji5_"
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\r\n",
        "clf_mlp = MLPClassifier(solver='lbfgs', alpha=1e-5,\r\n",
        "                    hidden_layer_sizes=(5, 2), random_state=1)\r\n",
        "clf_mlp.fit(X_train.relation_matrix, Y_train.values.flatten())\r\n",
        "    \r\n",
        "mlp_y_pred = clf_mlp.predict(Xtest)\r\n",
        "print(f1_score(Ytest, mlp_y_pred, average=None))\r\n",
        "if (class_count <= 2):\r\n",
        "  fpr, tpr, _ = roc_curve(Ytest, mlp_y_pred.flatten())\r\n",
        "  print(auc(fpr, tpr))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aH64V-INJ58g"
      },
      "source": [
        "refine_with_unexpectedness(test_data_set, Y_train.item_dict, mlp_y_pred, Ytest, refined_unexpected_rules, \"MLP\", clus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOoDC6vmgOFG"
      },
      "source": [
        "unique, counts = np.unique(Y_train.values, return_counts=True)\r\n",
        "\r\n",
        "print(np.asarray((unique, counts)).T)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poUk7HOTgpAw"
      },
      "source": [
        "Ytest"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwbCVLYhgkai"
      },
      "source": [
        "unique, counts = np.unique(Ytest, return_counts=True)\r\n",
        "\r\n",
        "print(np.asarray((unique, counts)).T)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}